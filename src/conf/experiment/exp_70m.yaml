true_rm: s-nlp/roberta_toxicity_classifier
learn_rm_size: &learn_rm_size 70m
learn_rm: &learn_rm EleutherAI/pythia-
from_checkpoint: false # false or path/to/ckpt
learn_rm_path: saved_models/learned_rm_ #${learn_rm}_${true_rm}.pt
policy_lm: jaredjoss/pythia-70m-roberta-lr_3e6-kl_0035-steps_600-rlhf-model
org_lm: EleutherAI/pythia-70m
evaluation_dataset: jaredjoss/jigsaw-long-2000
data_name: jaredjoss/jigsaw-long-2000
non_toxic_rm: *learn_rm
candidate_policies:
- jaredjoss/pythia-70m-roberta-lr_3e6-kl_0035-steps_600-rlhf-model
- *learn_rm
candidate_policy_paths:
- skrishna/jaredjoss-jigsaw-long-2000_70M_non_toxic
- skrishna/jaredjoss-jigsaw-long-2000_70M_toxic
create_dataset: false
correlation_threshold: false # false or {0, 1}
training:
  evaluation_metric: mse
  sample_size : 100
  loss_type: max_margin
  n_epochs: 30
  optimizer: adam
  lr: 0.00001
experiments_base_dir: ${oc.env:EXPERIMENT_DIR,/content/drive/MyDrive}
